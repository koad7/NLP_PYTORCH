{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjqE/t7C1tVpYqbilVl9TN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koad7/NLP_PYTORCH/blob/main/KnowledgeGraphAnd_TransformerNER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw8bZU5AfJ-u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data collection\n",
        "The first step is to collect a dataset of text that contains geographic locations. This dataset can be collected from a variety of sources, such as news articles, social media posts, and travel websites.\n",
        "\n",
        "\n",
        "### Data Sources\n",
        "[HarveyNER](https://github.com/brickee/harveyner) with fine-grained locations annotated in tweets.\n",
        "\n",
        " [OntoNotes](https://github.com/yanqiangmiffy/OntoNotes-5.0-NER-BIO) This is a CoNLL-2003 formatted version with BIO tagging scheme of the OntoNotes 5.0 release for NER in 3 languages (English, Arabic, Chinese). This formatted version is based on the instructions here and a new script created in this repo.\n",
        "\n",
        " [CoNLL-2003](https://paperswithcode.com/dataset/conll-2003) is a named entity recognition dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition. The data consists of eight files covering two languages: English and German. For each of the languages there is a training file, a development file, a test file and a large file with unannotated data.\n"
      ],
      "metadata": {
        "id": "WsAB63w1fh49"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6XV-bUKfsL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data cleaning:\n",
        "The next step is to clean the dataset of text. This includes removing any noise from the text, such as punctuation, stop words, and hyperlinks.\n",
        "\n"
      ],
      "metadata": {
        "id": "uWsn8IaLfs2j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ZFgFviWfw6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature extraction:\n",
        "The third step is to extract features from the text. These features can be used to train the NER model. Some common features that are used for NER tasks include word unigrams, bigrams, and trigrams, as well as part-of-speech tags."
      ],
      "metadata": {
        "id": "3kCkDXVKfyq4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWSlfHK_fx21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Knowledge graph:\n",
        "The knowledge graph of geographic entities can be used to disambiguate geographic locations in text and to provide context for the NER model. The knowledge graph can be created manually or by using a machine learning algorithm to extract entities and relationships from a dataset of text.\n",
        "\n",
        "**Creating/Obtaining a Knowledge Graph**: For geotagging, you might want to use a knowledge graph that contains detailed geographical information. GeoNames could be a good source for this. It's a geographical database that covers all countries and contains over eleven million placenames. If you're creating a knowledge graph from scratch, ensure that it contains relevant geographical entities and relationships that are pertinent to your task.\n"
      ],
      "metadata": {
        "id": "tb-lFH2Nf4TK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3k9Gw-J_f79V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BERT transformer:\n",
        "The BERT transformer is a deep learning model that can be used to learn the representations of words and phrases in text. The BERT transformer can be used as a starting point for training the NER model, which can save time and improve the accuracy of the model.\n",
        "\n",
        "\n",
        "[GeoBERT](https://www.mdpi.com/2076-3417/12/24/12942): This model was developed by Yves Scherrer and Nikola Ljubešić. It is a fine-tuned BERT model that is specifically designed for geotagging.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[HierBERT](https://www.timesofisrael.com/over-80-years-after-pearl-harbor-sailor-herbert-jacobson-to-be-laid-to-rest/): This model was developed by Kateryna Lutsai and Christoph H. Lampert. It is a hierarchical BERT model that uses both textual and temporal information to predict geolocations.\n",
        "\n",
        "\n",
        "\n",
        "[Geo-BERT-multilingual](https://huggingface.co/k4tel/geo-bert-multilingual): This model was developed by k4tel. It is a multilingual BERT model that can be used to geotag text in multiple languages.\n",
        "\n",
        "\n",
        "\n",
        "**Choosing the Right Transformer Model**: For a geotagging task, BERT (Bidirectional Encoder Representations from Transformers) could be a good choice due to its bidirectional nature, which allows it to understand the context from both directions (left and right of a word). However, you might also consider location-specific models if available. When choosing a model, consider factors like the amount of training data you have, the computational resources available, and the complexity of the task. Pre-trained models on a large corpus of text could be beneficial as they already understand the language's syntax and semantics to a certain extent.\n",
        "\n",
        "\n",
        "**Integrating the Knowledge Graph with the Transformer Model**: You plan to use graph neural networks to encode the knowledge graph into a dense vector. This is a good approach as it allows you to capture the relationships between entities in the knowledge graph. However, the integration process can be challenging. You might need to experiment with different ways of combining the knowledge graph embeddings with the transformer model's outputs. For example, you could try concatenating the embeddings, adding them, or using them as additional input at each layer of the transformer model.\n",
        "\n",
        "\n",
        "**Training the Model**: Training involves fine-tuning the transformer model's parameters on a labeled NER dataset. Make sure your dataset is large and diverse enough to capture the variety of geographical entities and contexts in which they can appear. Also, consider using techniques like data augmentation to increase the size and diversity of your training data. For example, you could use translation to and from another language to create new training examples"
      ],
      "metadata": {
        "id": "fizUpImgf8a0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tt5s6vJrgCDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-tuning:\n",
        "The NER model can be fine-tuned on a small dataset of labeled text to further improve its accuracy. The fine-tuning process involves adjusting the parameters of the model to better fit the labeled data.\n"
      ],
      "metadata": {
        "id": "Jh-iEygEgCkt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6u2hdVFgFbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation:\n",
        "The NER model can be evaluated on a held-out dataset of text to measure its accuracy. The evaluation results can be used to improve the model by tuning the hyperparameters or by collecting more training data.\n"
      ],
      "metadata": {
        "id": "HLY_XJ7sgF7s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKLo5tOYgI4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deployment:\n",
        "The NER model can be deployed to a web service or a mobile app. The deployed model can be used to extract geographic locations from text in real time."
      ],
      "metadata": {
        "id": "IcsWVd-sgJWB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YCt4lZoIgLWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is needed"
      ],
      "metadata": {
        "id": "2iY1hYdDqC3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a Named Entity Recognition (NER) system that uses a BERT transformer and a knowledge graph of geographic entities to improve the accuracy of geographic location recognition in text is a complex task. Here are the main building blocks of such a system:\n",
        "\n",
        "1. **BERT Model**: You would need a pre-trained BERT model as the base of your system. This model can be fine-tuned on a NER task. There are several pre-trained BERT models available that you can use, such as BERT-Base, BERT-Large, etc.\n",
        "\n",
        "2. **NER Dataset**: You would need a labeled dataset for the NER task. This dataset should contain text with annotated geographic locations. You can use this dataset to fine-tune the BERT model.\n",
        "\n",
        "3. **Knowledge Graph**: You would need a knowledge graph of geographic entities. This knowledge graph should contain information about various geographic locations. You can use this knowledge graph to enrich the information that the BERT model has about geographic locations.\n",
        "\n",
        "4. **Knowledge Graph Embedding**: You would need a method to embed the knowledge graph into a vector space that can be used by the BERT model. There are several methods available for this, such as TransE, TransR, etc.\n",
        "\n",
        "5. **Integration Method**: You would need a method to integrate the knowledge graph embeddings with the BERT model. This could be done in several ways, such as by concatenating the embeddings with the BERT output, or by using the embeddings as additional input to the BERT model.\n",
        "\n",
        "6. **Training Procedure**: You would need a procedure to train the integrated model. This could involve fine-tuning the BERT model on the NER task, while also updating the knowledge graph embeddings based on the training data.\n",
        "\n",
        "7. **Evaluation Procedure**: You would need a procedure to evaluate the performance of the model. This could involve measuring the precision, recall, and F1 score of the model on a test set.\n",
        "\n",
        "8. **Inference Procedure**: Finally, you would need a procedure to use the trained model to perform NER on new, unseen text.\n",
        "\n",
        "Remember, building such a system is a complex task that requires a good understanding of both NLP and knowledge graphs. It's also a research task, so there's no guarantee that the final system will perform as expected. However, it's a very interesting project that could lead to a significant improvement in the accuracy of NER for geographic locations."
      ],
      "metadata": {
        "id": "NAbLaTAzp8n0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bj4bzQUayd7s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVBE0Fqmp-MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "rXJv3hvgwTMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "-yKWLUqAwWcw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e_1McAghtfpJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}